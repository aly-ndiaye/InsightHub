# Data Model — InsightHub

## 1. Overview
The InsightHub data model defines how datasets, their transformations, and their performance indicators are structured and tracked.  
It ensures traceability, observability, and analytical readiness across the platform — from ingestion to business intelligence.

The schema follows a **star-like design** centered on the `datasets` table, connected to supporting entities such as `events`, `metrics`, and `users`.

---

## 2. Entities

### A. datasets — Central table
Purpose: reference each data source monitored by the platform.

Fields:
- id – unique identifier  
- name – dataset name (unique)  
- source – origin of the data (API, CSV, provider, etc.)  
- frequency – expected update frequency (daily, weekly, monthly, ad-hoc)  
- last_updated – timestamp of the last successful ingestion  
- status – active / inactive / warning  
- created_at, updated_at – audit timestamps  

---

### B. events — Operational log
Purpose: record all actions or transformations that occur on a dataset.

Fields:
- id – unique identifier  
- dataset_id – foreign key referencing `datasets`  
- event_type – ingestion, transformation, validation, error, publication  
- timestamp – when the event occurred  
- details – metadata or message (e.g., processed rows, error logs)  
- user_id – optional foreign key referencing `users` (who triggered the event)

---

### C. metrics — Data quality and performance measurements
Purpose: store **historical KPIs** describing the health and behavior of each dataset.

Fields:
- id – unique identifier  
- dataset_id – foreign key referencing `datasets`  
- metric_name – freshness, completeness, ingestion_latency, error_rate, record_count, etc.  
- value – numeric value (e.g., 0.97, 4.7, 123456)  
- collected_at – timestamp of measurement  
- notes (optional) – contextual information (e.g., "SLA breached: upstream delay")

Each ETL or ingestion run can generate multiple metric entries per dataset.  
This enables **time series analysis** on data freshness, quality, and volume.

---

### D. users — Audit and roles (optional)
Purpose: identify actors or services interacting with the platform.

Fields:
- id – unique identifier  
- username – name or service ID  
- role – admin / analyst / viewer / service  
- last_login – timestamp of last connection

---

## 3. Relationships

- One dataset → many events  
- One dataset → many metrics  
- One user → many events (if user tracking is enabled)

---

## 4. Schema Overview (Text-based ERD)

            +---------+
            |  users  |
            | id      |
            | role    |
            +----+----+
                 |
                 | (1-N)
                 |
+----------------v----------------+
|            events               |
| id           dataset_id  ---> datasets.id
| event_type   user_id     ---> users.id (opt)
| timestamp    details
+-------+-------------------------+
        |
        | (N-1)
        |
+-------v-------------------------+
|            datasets             |
| id   name   source   frequency  |
| last_updated   status           |
| created_at     updated_at       |
+-------+-------------------------+
        |
        | (1-N)
        |
+-------v-------------------------+
|            metrics              |
| id    dataset_id  ---> datasets.id
| metric_name  value  collected_at
| notes (opt)
+-------------------------------+

---

## 5. Rationale

- **Traceability:** `events` provides a historical log of every operation.  
- **Observability:** `metrics` quantifies data quality, freshness, and ingestion performance.  
- **Analytical readiness:** star-like schema facilitates reporting and dashboarding in Power BI or Streamlit.  
- **Scalability:** new KPIs, event types, or user actions can be added without breaking the model.

---

## 6. Future Extensions

Possible future entities:
- `models` – machine learning models trained on datasets.  
- `predictions` – inference results generated by those models.  
- `alerts` – automated notifications based on KPI thresholds.

This model can evolve toward a full **data observability and intelligence platform**, supporting continuous monitoring, analytics, and automation.
